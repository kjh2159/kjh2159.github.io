---
title: 안드로이드에서 llama.cpp 구동
date: 2024-12-24 09:00:00 +0900
categories: [llama.cpp, llm]
tags: [llama.cpp, llm, mobile, android, ai, gguf, github, termux]
---

어쩌면, 이 방식이 llama.cpp를 구동시키는 가장 간단한 방식이라 생각한다. CLI 버전으로 실행시킬 거다. 다른 실행법을 원한다면, 다른 글 참고 부탁...

# 1. llama.cpp

llama.cpp는 무엇인가? 다양한 Backend 상에서 LLM을 구동시킬 수 있는 엔진이다. 이 llama.cpp 엔진과 호환되는 파일 포맷은 .gguf이다.
> llama.cpp github 주소: [llama.cpp](https://github.com/ggerganov/llama.cpp) 
> gguf 설명: [gguf](https://huggingface.co/docs/hub/en/gguf)

# 2. Termux 설치

먼저 llama.cpp를 Android OS 상에서는 Termux를 통해 구동한다. Play Store에서 ***Termux***를 설치해준다.


# 3. 패키지 설치

일단 업뎃 및 필요 기능 설치

```bash
apt update && apt upgrade -y
apt install git cmake vim
```

# 4. pyenv 설치 (옵션)

이 부분은 pyenv 설치방법인데 추후 pyenv를 사용할 때가 있어서 설치했다. pyenv를 안 쓰거나 python을 활용할 계획이 없다면, 설치하지 않아도 된다.

```bash
curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash
vim ~/.bashrc
```

아래 코드를 `.bashrc`에 붙여넣는다.

```
export PATH="~/.pyenv/bin:$PATH"
eval "$(pyenv init -)"
eval "$(pyenv virtualenv-init -)" 
```

적용

``bash
source ~/.bashrc
``

추가 패키지 설치

```bash
pkg install build-essential gdbm libandroid-posix-semaphore libandroid-support libbz2 libcrypt libexpat libffi liblzma libsqlite ncurses ncurses-ui-libs openssl readline zlib
```

# 5. git clone

llama.cpp를 클로닝해준다.

```
git clone https://github.com/ggreganov/llama.cpp.git
cd llama.cpp
```

# 6. Build

빌드는 아래의 명령어로 가능. 아래의 명령어는 `CURL` 옵션으로 모델 파일의 자동 다운을 허용시키는 옵션이다. 

```bash
cmake -S . -B build -DLLAMA_CURL=ON
cmake --build build --config Release
```

물론 아래의 명령어가 기본 빌드 명령어다. 아래 명령어를 사용하면 실행시 지정 디렉토리에 model 파일이 있어야 한다. 근데, 그건 안드로이드 CLI 특성상 매우 귀찮기에 그냥 CURL 옵션 ON을 해줬다. 그래서 위 명령어 추천

```bash
cmake -B build
cmake --build build --config Release
```

더 많은 옵션은 이 [페이지](https://github.com/ggerganov/llama.cpp/blob/master/docs/master/docs/build.md)를 참고.

# 7. 실행

실행 명령어는 첫 번째 빌드 명령어 기준이다.

```bash
./build/bin/llama-cli \
--hf-repo bartowski/Llama-3.2-1B-Instruct-GGUF \
--hf-file Llama-3.2-1B-Instruct-Q4_0.gguf \
-p "hello" -cnv
```

> 명령어 설명 \
> --hf-repo : bartowski의 Llama-3.2-1B-Instruct-GGUF라는 hugging face repository \
> --hf-file : 해당 repository의 Llama-3.2-1B-Instruct-Q4_0.gguf라는 파일 \
> -p : "hello" 라는 이름의 prompt \
> -cnv : conversation mode \
{: .prompt-info}

추가적인 설명을 덧붙이자면, 원래 Android에서 실행가능한 Quantization은 `Q4_0_N_M` 형식이 유일했다. 그런데 이 Quantization이 llama.cpp에서는 기존의 `Q4_0` Quantization으로 Refactoring 및 병합되어 Llama-3.2-1B-Instruct-Q4_0만이 호환된다. 이전 버전의 llama.cpp를 사용한다면, `Q4_0`가 아닌 `Q4_0_4_4`만 동작할 수도 있다.